# Attention-based Seq2Seq Configuration

logging:
  level: "INFO"
  file: "logs/app.log"

data:
  n_train: 2000
  n_test: 500
  src_length: 6
  tgt_length: 6
  vocab_size: 12
  random_seed: 42

model:
  d_model: 16
  d_k: 16
  d_v: 16

training:
  epochs: 15
  learning_rate: 0.05
  batch_size: 32

